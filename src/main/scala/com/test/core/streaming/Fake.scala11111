package com.test.core.streaming

import org.apache.log4j.Logger
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
import org.apache.spark.streaming.Milliseconds
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.sql.hive.HiveContext

object Fake {
def main(args: Array[String]) {
    if (args.length < 4) {
      System.err.println(
        "Usage: KafkaWordCountProducer <metadataBrokerList> <topic> " +
          "<messagesPerSec> <wordsPerMessage>")
      System.exit(1)
    }
val sparkConf = new SparkConf().setMaster("local[2]").setAppName("Fake")
val sc = new SparkContext(sparkConf)
val hiveCtx = new org.apache.spark.sql.hive.HiveContext(sc)
val rows = hiveCtx.sql("SELECT chromeid, readcount FROM genome1")
val firstRow = rows.first()
println(firstRow.getString(0))

println("Hello world")

}
}
